#!/usr/bin/which python

from __future__ import print_function, division

import sys, os, os.path, glob, tempfile, shutil
import logging

from argparse import ArgumentParser

import multiprocessing as mp

import numpy as np
import scipy.stats

import pysam

from genome_tools import bed, genomic_interval 
from footprint_tools.stats import mutual_information

# 
#from scipy.ndimage.filters import median_filter
from scipy.ndimage.measurements import label, find_objects


logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
logger = logging.getLogger(__name__)


def parse_options(args):

    parser = ArgumentParser(description = "Compute the mutual information between footprinted bases from posteriors")

    parser.add_argument("interval_file", metavar = "interval_file", type = str, 
                        help = "File path to BED file")

    parser.add_argument("posterior_file", metavar = "posterior_file", type = str,
                        help = "Path to TABIX file containing per-nucleotide posterior p-values")

    grp_st = parser.add_argument_group("statistical options")

    #grp_st.add_argument("--filter-width", metavar = "FILTER_WIDTH", type = int,
    #                    dest = "filter_width", default = 3,
    #                    help = "Width of filter to smooth mutual information score matrix")

    grp_st.add_argument("--mi-cutoff", metavar = "N", type = float,
                        dest = "mi_cutoff", default = 0.1,
                        help = "Only consider nucleotides with FDR <= this value."
                        " (default: %(default)s)")

    grp_st.add_argument("--min-contig-bases", metavar = "N", type = int,
                        dest = "min_contig_bases", default = 9,
                        help = "Only output regions containing this amount of contiguous signal"
                        " (default: %(default)s)")

    #grp_st.add_argument("--max-interval-width", metavar = "N", type = int,
    #                    dest = "max_interval_width", default = 25000,
    #                    help = "Only compute MI for regions containing less than or equal to this width"
    #                    " (default: %(default)s)")

    grp_ot = parser.add_argument_group("other options")

    grp_ot.add_argument("--processors", metavar = "N", type = int,
                        dest = "processors", default = mp.cpu_count(),
                        help = "Number of processors to use."
                        " (default: all available processors)")

    return parser.parse_args(args)

def process_func(queue, outfile, mi_cutoff = 0.1, min_contig_bases = 9):

    handle = open(outfile, "w")

    while 1:

        data = queue.get()

        if data == None:
            queue.task_done()
            break

        (interval, logp) = data

        p = np.exp(-logp)
        p[p<=0.0] = 1e-16
        p[p>1.0] = 1.0
        p[np.isnan(p)] = 1.0

        # digitize P-value matrix
        I = np.apply_along_axis(np.digitize, 0, p, bins = [0, 0.15, 0.85, 1], right = True) - 1

        #print(np.min(I), np.max(I))
        #print(np.min(p), np.max(p))

        # compute mutual information
        M = mutual_information.mutual_information(np.array(I, dtype = np.intc, order = 'c'))

        #compute spearman's rho (only used for colors)
        #(rho, p_rho) = scipy.stats.spearmanr(I, axis = 0)

        #Z = median_filter(M, size = (filter_width, filter_width))
        
	labeled_array, num_features = label(M >= mi_cutoff)

        res = ""

        for o in find_objects(labeled_array):
           
            x0 = o[0].start
            x1 = o[0].stop
            y0 = o[1].start
            y1 = o[1].stop
            
            wa = x1-x0
            wb = y1-y0
            
            if x0 >= y0:
                continue

            if wa*wb < min_contig_bases:
                continue

            #if np.median(rho[x0:x1,y0:y1]) > 0:
            #    score = 100
            #    col = "255,0,0"
            #else:
            #    score = -100
            #    col = "0,0,255"

            score = np.max(M[x0:x1, y0:y1])
            col = "0,0,255"

            res += "%s\t%d\t%d\t.\t%0.4f\t.\t%d\t%d\t%s\t2\t%d,%d\t0,%s\n" % (interval.chrom, interval.start+x0, interval.start+y1, score, interval.start+x0, interval.start+y1, col, wa, wb, y0-x0)

        print(res, file = handle, end = "")

        queue.task_done()

    handle.close()

class process_callback(object):

    def __init__(self, filehandle = sys.stdout):

        self.filehandle = filehandle

    def __call__(self, res):

        print(res, end = "")

def main(argv = sys.argv[1:]):

    args = parse_options(argv)

    # Load intervals file
    intervals = genomic_interval.genomic_interval_set(bed.bed3_iterator(open(args.interval_file)))
    n_intervals = len(intervals)

    # Read dimensions
    tabix_handle = pysam.TabixFile(args.posterior_file)
    row = tabix_handle.fetch(parser = pysam.asTuple()).next()
    n_datasets = len(row) - 3

    #
    tmpdir = tempfile.mkdtemp()
    chunk_files = [os.path.join(tmpdir, "chunk%s" % i) for i in range(args.processors-1)]

    q = mp.JoinableQueue()

    processors = [ mp.Process(target = process_func, args = (q, f, args.mi_cutoff, args.min_contig_bases)) for f in chunk_files ]
    [processor.start() for processor in processors]

    logger.info("Working (%d threads; chunked results in %s)" % (len(chunk_files), tmpdir))

    for interval in intervals:

        l = len(interval)

        ln_p = np.zeros((n_datasets, l), dtype = np.float64)

        try:
            for row in tabix_handle.fetch(interval.chrom, interval.start, interval.end, parser = pysam.asTuple()):
                j = int(row[1]) - interval.start
                ln_p[:,j] = np.array(row[3:], dtype = np.float64)
            q.put((interval, ln_p))
        except:
            pass    

        while q.qsize() > 500:
            pass

    logger.info("Finishing up final processing...")

    q.join() # Wait for queue to unblock
    [q.put(None) for i in range(len(chunk_files))] # sends a return command to processing threads
    [processor.join() for processor in processors] # wait for threads to stop
 
    logger.info("Merging data...")

    for file in chunk_files: 
        with open(file, 'r') as handle:
            for line in handle:
                sys.stdout.write(line)

    logger.info("Cleaning up...")

    tabix_handle.close()
    shutil.rmtree(tmpdir)

    return 0
    tabix.close()

    return 0

if __name__ == "__main__":
    sys.exit(main())
