#!/usr/bin/which python

from __future__ import print_function, division

import sys, os, os.path, glob, tempfile, shutil
import logging

from argparse import ArgumentParser

import multiprocessing as mp

from collections import Counter

import numpy as np
import scipy.stats

import pysam

from genome_tools import bed, genomic_interval 
from footprint_tools.stats import mutual_information, segment

logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level = logging.DEBUG)
logger = logging.getLogger(__name__)


def parse_options(args):

    parser = ArgumentParser(description = "Compute the mutual information between footprinted bases from posteriors")

    parser.add_argument("interval_file", metavar = "interval_file", type = str, 
                        help = "File path to BED file")

    parser.add_argument("posterior_file", metavar = "posterior_file", type = str,
                        help = "Path to TABIX file containing per-nucleotide posterior p-values")

    grp_st = parser.add_argument_group("statistical options")


    grp_st.add_argument("--fps-cutoff", metavar = "N", type = float,
                        dest = "fps_cutoff", default = 0.0001,
                        help = "Only consider fooptrints with a consensus p-value <= this value."
                        " (default: %(default)s)")

    grp_ot = parser.add_argument_group("other options")

    grp_ot.add_argument("--processors", metavar = "N", type = int,
                        dest = "processors", default = mp.cpu_count(),
                        help = "Number of processors to use."
                        " (default: all available processors)")
    
    grp_ot.add_argument("--tmpdir", metavar = "", type = str,
                        dest = "tmpdir", default = None,
                        help = "Temporary directory to use"
                        " (default: uses unix 'mkdtemp'")


    return parser.parse_args(args)

#def process_func(queue, outfile, mi_cutoff = 0.1, filter_width = 3, min_contig_bases = 16):
def process_func(queue, outfile, fps_cutoff = 0.0001):

    handle = open(outfile, "w")

    while 1:

        data = queue.get()

        if data == None:
            queue.task_done()
            break

        (interval, logp) = data

        fps = segment.segment(np.nanmax(logp, axis = 0), -np.log(fps_cutoff), 3)

        # No footprints; skip
        if len(fps) == 0:
            queue.task_done()
            continue

        logp_fps = np.zeros((logp.shape[0], len(fps)))
        
        for j, f in enumerate(fps):
            s = f[0]
            e = f[1]
            logp_fps[:,j] = np.max(logp[:,s:e], axis = 1)

        p = np.exp(-logp_fps)
        p[p<=0.0] = 1e-16
        p[p>1.0] = 1.0
        p[np.isnan(p)] = 1.0
        
        I = np.apply_along_axis(np.digitize, 0, p, bins = [0, 0.1, 1], right = True) - 1
        
        # Remove cell types if 0 footprints are called with in the interval
        k = np.apply_along_axis(np.min, 1, I) == 0
        I_reduced = np.array(I[k,:], dtype = np.intc, order = 'c')

        m, m_null, p_mi = mutual_information.mutual_information_p(I_reduced, bins = 2, ntimes = 10000)

        res = ""

        for i in range(p_mi.shape[0]):
            for j in range(i+1, p_mi.shape[0]):
            
                # remove the sites which permutation is unable come up with a reliable distribution
                # granted that this is a little bit of hack to help estimate the FDR better

                if len(set(m_null[i, j, :])) <= 3:
                    continue

                # determine directionality of association

                cnt = Counter((I_reduced[:,i] << 1) ^ I_reduced[:,j])
                if cnt[0] + cnt[3] > cnt[1] + cnt[2]:
                    col = "0,0,255"  
                else:
                    col = "0,255,0"

                x0 = fps[i][0]
                x1 = fps[i][1]
                y0 = fps[j][0]
                y1 = fps[j][1] 

                wa = x1-x0
                wb = y1-y0

                res += "%s\t%d\t%d\t.\t%0.4f\t.\t%d\t%d\t%s\t2\t%d,%d\t0,%s\n" % (interval.chrom, interval.start+x0, interval.start+y1, -np.log(p_mi[i, j]), interval.start+x0, interval.start+y1, col, wa, wb, y0-x0)

        print(res, file = handle, end = "")

        queue.task_done()

    handle.close()

class process_callback(object):

    def __init__(self, filehandle = sys.stdout):

        self.filehandle = filehandle

    def __call__(self, res):

        print(res, end = "")

def main(argv = sys.argv[1:]):

    args = parse_options(argv)

    # Load intervals file
    intervals = genomic_interval.genomic_interval_set(bed.bed3_iterator(open(args.interval_file)))
    n_intervals = len(intervals)

    # Read dimensions
    tabix_handle = pysam.TabixFile(args.posterior_file)
    row = tabix_handle.fetch(parser = pysam.asTuple()).next()
    n_datasets = len(row) - 3

    #
    tmpdir = tempfile.mkdtemp()
    chunk_files = [os.path.join(tmpdir, "chunk%s" % i) for i in range(args.processors-1)]

    q = mp.JoinableQueue()

    processors = [ mp.Process(target = process_func, args = (q, f, args.fps_cutoff)) for f in chunk_files ]

    [processor.start() for processor in processors]

    logger.info("Working (%d threads; chunked results in %s)" % (len(chunk_files), tmpdir))

    for interval in intervals:

        l = len(interval)

        ln_p = np.zeros((n_datasets, l), dtype = np.float64)

        try:
            for row in tabix_handle.fetch(interval.chrom, interval.start, interval.end, parser = pysam.asTuple()):
                j = int(row[1]) - interval.start
                ln_p[:,j] = np.array(row[3:], dtype = np.float64)
            
            q.put((interval, ln_p[:,:]))
        
        except:
            pass    

        while q.qsize() > 500:
            pass

    [q.put(None) for i in range(len(chunk_files))] # sends a return command to processing threads

    logger.info("Finishing up final processing...")

    q.join() # Wait for queue to unblock
    [processor.join() for processor in processors] # wait for threads to stop
 
    logger.info("Merging data...")

    for file in chunk_files: 
        with open(file, 'r') as handle:
            for line in handle:
                sys.stdout.write(line)

    logger.info("Cleaning up...")

    tabix_handle.close()
    shutil.rmtree(tmpdir)

    return 0

if __name__ == "__main__":
    sys.exit(main())
