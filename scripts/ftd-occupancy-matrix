
#!/usr/bin/which python

from __future__ import print_function, division

import sys, os, os.path, glob, tempfile, shutil
import logging

from argparse import ArgumentParser

import multiprocessing as mp

from collections import Counter

import numpy as np
import scipy.stats

import pysam

from genome_tools import bed, genomic_interval 
from footprint_tools.stats import mutual_information, segment

logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level = logging.DEBUG)
logger = logging.getLogger(__name__)


def parse_options(args):

    parser = ArgumentParser(description = "Create a binary occupancy matrix from posteriors")

    parser.add_argument("interval_file", metavar = "interval_file", type = str, 
                        help = "File path to BED file contain regions to evaluate footprints")

    parser.add_argument("posterior_file", metavar = "posterior_file", type = str,
                        help = "Path to TABIX file containing per-nucleotide posterior p-values")

    grp_st = parser.add_argument_group("statistical options")

    grp_st.add_argument("--fps-cutoff", metavar = "N", type = float,
                        dest = "fps_cutoff", default = 0.0001,
                        help = "Only consider fooptrints with a consensus p-value <= this value."
                        " (default: %(default)s)")

    grp_ot = parser.add_argument_group("other options")

    grp_ot.add_argument("--processors", metavar = "N", type = int,
                        dest = "processors", default = mp.cpu_count(),
                        help = "Number of processors to use."
                        " (default: all available processors)")
    
    grp_ot.add_argument("--tmpdir", metavar = "", type = str,
                        dest = "tmpdir", default = None,
                        help = "Temporary directory to use"
                        " (default: uses unix 'mkdtemp'")


    return parser.parse_args(args)


def process_func(queue, outfile, fps_cutoff = 0.0001):

    handle = open(outfile, "w")

    while 1:

        data = queue.get()

        if data == None:
            queue.task_done()
            break

        (interval, logp) = data

        fps = segment.segment(np.nanmax(logp, axis = 0), -np.log(fps_cutoff), 3)

        # No footprints; skip
        if len(fps) == 0:
            queue.task_done()
            continue

        logp_fps = np.zeros((logp.shape[0], len(fps)))
        
        for j, f in enumerate(fps):
            s = f[0]
            e = f[1]
            logp_fps[:,j] = np.max(logp[:,s:e], axis = 1)

        p = np.exp(-logp_fps)
        p[p<=0.0] = 1e-16
        p[p>1.0] = 1.0
        p[np.isnan(p)] = 1.0

        I = np.apply_along_axis(np.digitize, 0, p, bins = [0, 0.1, 1], right = True) - 1

        res = ""
        
        for i in range(I.shape[1]):

            start = fps[i][0] + interval.start
            end = fps[i][0] + interval.start

            res += "{}\t{:d}\t{:d}\t{}:{:d}-{:d}\t".format(interval.chrom, fps[i][0] + interval.start, fps[i][1] + interval.start, interval.chrom, interval.start, interval.end) + "\t".join(map(lambda x: str(int(x)), I[:,i])) + "\n"

        print(res, file = handle, end = "")

        queue.task_done()

    handle.close()

def main(argv = sys.argv[1:]):

    args = parse_options(argv)

    # Load intervals file
    intervals = genomic_interval.genomic_interval_set(bed.bed3_iterator(open(args.interval_file)))
    n_intervals = len(intervals)

    # Read dimensions
    tabix_handle = pysam.TabixFile(args.posterior_file)
    row = tabix_handle.fetch(parser = pysam.asTuple()).next()
    n_datasets = len(row) - 3

    #
    tmpdir = tempfile.mkdtemp()
    chunk_files = [os.path.join(tmpdir, "chunk%s" % i) for i in range(args.processors-1)]

    q = mp.JoinableQueue()

    processors = [ mp.Process(target = process_func, args = (q, f, args.fps_cutoff)) for f in chunk_files ]

    [processor.start() for processor in processors]

    logger.info("Working (%d threads; chunked results in %s)" % (len(chunk_files), tmpdir))

    for interval in intervals:

        l = len(interval)

        ln_p = np.zeros((n_datasets, l), dtype = np.float64)

        try:
            for row in tabix_handle.fetch(interval.chrom, interval.start, interval.end, parser = pysam.asTuple()):
                j = int(row[1]) - interval.start
                ln_p[:,j] = np.array(row[3:], dtype = np.float64)
            
            q.put((interval, ln_p[:,:]))
        
        except:
            pass    

        while q.qsize() > 500:
            pass

    [q.put(None) for i in range(len(chunk_files))] # sends a return command to processing threads

    logger.info("Finishing up final processing...")

    q.join() # Wait for queue to unblock
    [processor.join() for processor in processors] # wait for threads to stop
 
    logger.info("Merging data...")

    for file in chunk_files: 
        with open(file, 'r') as handle:
            for line in handle:
                sys.stdout.write(line)

    logger.info("Cleaning up...")

    tabix_handle.close()
    shutil.rmtree(tmpdir)

    return 0

if __name__ == "__main__":
    sys.exit(main())

